import os, json, multiprocessing, difflib 
from typing import AsyncGenerator, Literal, List
from pydantic import BaseModel
from llama_cpp import Llama

# âœ… CBT1 ëª¨ë¸ ìºì‹œ
LLM_CBT1_INSTANCE = {}

def load_cbt1_model(model_path: str) -> Llama:
    global LLM_CBT1_INSTANCE
    if model_path not in LLM_CBT1_INSTANCE:
        print(f"ğŸ“¦ CBT1 ëª¨ë¸ ë¡œë”©: {model_path}", flush=True)
        NUM_THREADS = max(1, multiprocessing.cpu_count() - 1)
        LLM_CBT1_INSTANCE[model_path] = Llama(
            model_path=model_path,
            n_ctx=1024,
            n_threads=NUM_THREADS,
            n_batch=8,
            max_tokens=128,
            temperature=0.95,
            top_p=0.92,
            presence_penalty=1.4,
            frequency_penalty=1.2,
            repeat_penalty=1.3,
            n_gpu_layers=0,
            low_vram=True,
            use_mlock=False,
            verbose=False,
            chat_format="llama-3",
            stop=["<|im_end|>"]
        )
    return LLM_CBT1_INSTANCE[model_path]

# âœ… ìƒíƒœ ëª¨ë¸
class AgentState(BaseModel):
    stage: Literal["cbt1", "cbt2"]
    question: str
    response: str
    history: List[str]
    turn: int

def get_cbt1_prompt(enhanced=False) -> str:
    prompt = (
        "ë‹¹ì‹ ì€ ìë™ì‚¬ê³ ë¥¼ íƒìƒ‰í•˜ëŠ” ë”°ëœ»í•˜ê³  ì´ì„±ì ì¸ CBT ìƒë‹´ìì…ë‹ˆë‹¤.\n"
        "- ì‚¬ìš©ìì˜ ë§ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ì‚¬ê³ ë¥¼ ë„ì™€ì£¼ì‹œê³ , í•­ìƒ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.\n"
        "- ë°˜ë“œì‹œ í•œ ë¬¸ì¥ ë˜ëŠ” ë‘ ë¬¸ì¥ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•´ ì£¼ì„¸ìš”.\n"
        "- ê°™ì€ í‘œí˜„, ë§íˆ¬, ì–´ë¯¸, ë¬¸ì¥ êµ¬ì¡°, ë‹¨ì–´ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³ , ë§¤ë²ˆ ë‹¤ë¥´ê²Œ í‘œí˜„í•´ ì£¼ì„¸ìš”.\n"
        "- ê°ì •, ê·¼ê±°, ì¥ê¸°ì  ê²°ê³¼, íƒ€ì¸ì˜ ì‹œê°, ë°˜ë³µëœ íŒ¨í„´, ì˜ˆì™¸ì  ìƒí™© ë“± ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.\n"
        "- ì˜ˆì‹œ: 'ê·¸ë•Œ ê°€ì¥ ê°•í•˜ê²Œ ëŠë‚€ ê°ì •ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?', 'ê·¸ ìƒê°ì„ ê³„ì† ë¯¿ìœ¼ë©´ ì–´ë–¤ ì˜í–¥ì´ ìƒê¸¸ê¹Œìš”?', 'ì´ì „ê³¼ ë¹„ìŠ·í•œ ìƒí™©ì´ ë°˜ë³µëœ ì  ìˆë‚˜ìš”?'"
    )
    if enhanced:
        prompt += (
            "\n- ìµœê·¼ ëŒ€í™” íë¦„ì´ ë°˜ë³µë˜ì—ˆê±°ë‚˜ ë°©í–¥ì´ ëª¨í˜¸í–ˆìŠµë‹ˆë‹¤. ì´ì „ë³´ë‹¤ ë”ìš± êµ¬ì²´ì ì´ê³  ì´ì „ê³¼ëŠ” ë‹¤ë¥¸ ì‹œê°ì„ ì œê³µí•˜ëŠ” ì§ˆë¬¸ì„ ì‹œë„í•´ ë³´ì„¸ìš”."
        )
    return prompt

# âœ… CBT1 ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
async def stream_cbt1_reply(state: AgentState, model_path: str) -> AsyncGenerator[bytes, None]:
    user_input = state.question.strip()
    history = state.history or []

    print(f"ğŸ§  [CBT1 í˜„ì¬ í„´: {state.turn}]")

    if not user_input:
        fallback = "ë– ì˜¤ë¥¸ ìƒê°ì´ë‚˜ ê°ì •ì´ ìˆë‹¤ë©´ í¸í•˜ê²Œ ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”."
        state.response = fallback
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt1",
            "turn": state.turn,
            "response": fallback,
            "question": "",
            "history": history
        }, ensure_ascii=False).encode("utf-8")
        return

    try:
        llm = load_cbt1_model(model_path)
        enhanced = any(s == "cbt1" and d for s, d in getattr(state, "drift_trace", [])[-5:])
        system_prompt = get_cbt1_prompt(enhanced)
        messages = [{"role": "system", "content": system_prompt}]
        for i in range(0, len(history), 2):
            if i + 1 < len(history):
                messages.append({"role": "user", "content": history[i]})
                messages.append({"role": "assistant", "content": history[i + 1]})
        messages.append({"role": "user", "content": user_input})

        full_response = ""
        first_token_sent = False
        for chunk in llm.create_chat_completion(messages=messages, stream=True):
            token = chunk["choices"][0]["delta"].get("content", "")
            if token:
                full_response += token
                if not first_token_sent:
                    yield b"\n"
                    first_token_sent = True
                yield token.encode("utf-8")

        reply = full_response.strip() or "ì¢‹ì•„ìš”. ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì´ì•¼ê¸°í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"
        state.response = reply

        for past in history[-10:]:
            if isinstance(past, str):
                if difflib.SequenceMatcher(None, reply[:40], past[:40]).ratio() > 0.8:
                    reply += " ê·¸ë¬êµ°ìš”, ê·¸ê²Œ ì •ë§ ì‚¬ì‹¤ì¼ê¹Œìš”? ì™œê³¡ë˜ì§€ëŠ” ì•Šì•˜ë‚˜ìš”?"
                    break

        next_turn = state.turn + 1
        next_stage = "cbt2" if next_turn >= 5 else "cbt1"

        updated_history = history.copy()
        if not (len(updated_history) >= 2 and updated_history[-2] == user_input and updated_history[-1] == reply):
            updated_history.extend([user_input, reply])

        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": next_stage,
            "turn": 0 if next_stage == "cbt2" else next_turn,
            "response": reply,
            "question": "",
            "history": updated_history
        }, ensure_ascii=False).encode("utf-8")

    except Exception as e:
        print(f"âš ï¸ CBT1 ì˜¤ë¥˜: {e}", flush=True)
        fallback = "ì£„ì†¡í•´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        state.response = fallback
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt1",
            "turn": state.turn,
            "response": fallback,
            "question": "",
            "history": history
        }, ensure_ascii=False).encode("utf-8")
