import os, json, multiprocessing
from typing import AsyncGenerator, Literal, List, Tuple
from pydantic import BaseModel
from llama_cpp import Llama

LLM_MI_INSTANCE = {}

def load_mi_model(model_path: str) -> Llama:
    global LLM_MI_INSTANCE
    if model_path not in LLM_MI_INSTANCE:
        try:
            print("ğŸš€ MI ëª¨ë¸ ë¡œë”© ì¤‘...", flush=True)
            LLM_MI_INSTANCE[model_path] = Llama(
                model_path=model_path,
                n_ctx=512,
                n_threads=max(1, multiprocessing.cpu_count() - 1),
                n_batch=4,
                max_tokens=128,
                temperature=0.7,
                top_p=0.85,
                top_k=40,
                repeat_penalty=1.1,
                frequency_penalty=0.7,
                presence_penalty=0.5,
                n_gpu_layers=0,
                low_vram=True,
                use_mlock=False,
                verbose=False,
                chat_format="llama-3",
                stop=["<|im_end|>", "\n\n"]
            )
            print("âœ… MI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ", flush=True)
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}", flush=True)
            raise RuntimeError("MI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
    return LLM_MI_INSTANCE[model_path]

class AgentState(BaseModel):
    question: str
    response: str
    history: List[str]
    drift_trace: List[Tuple[str, bool]] = []

def get_mi_prompt(context="empathy", enhanced=False) -> str:
    if context == "empathy":
        prompt = (
            "ë‹¹ì‹ ì€ ê³µê°ì ì´ê³  ì§€ì§€ì ì¸ ìƒë‹´ìì…ë‹ˆë‹¤.\n"
            "- ì‚¬ìš©ìì˜ ê°ì •ê³¼ ì–´ë ¤ì›€ì„ ê³µê°í•˜ë©´ì„œ, ë¬¸ì œ ì¸ì‹ì„ ë„ì™€ì£¼ì„¸ìš”.\n"
            "- ì‚¬ìš©ìê°€ ìì‹ ì˜ ìƒí™©ì„ ë˜ëŒì•„ë³¼ ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.\n"
            "- ì˜ˆ: 'ì§€ê¸ˆ ê°€ì¥ í˜ë“  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?', 'ë§ˆìŒì†ì—ì„œ ì–´ë–¤ ê°ì •ì´ ì˜¤ê°€ê³  ìˆë‚˜ìš”?'"
        )
    else:
        prompt = (
            "ë‹¹ì‹ ì€ ì–‘ê°€ê°ì •ì„ ë‹¤ë£¨ëŠ” ìƒë‹´ìì…ë‹ˆë‹¤.\n"
            "- ì‚¬ìš©ìëŠ” ë³€í™”ì˜ í•„ìš”ì„±ì„ ì¸ì‹í–ˆì§€ë§Œ, ì£¼ì €í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
            "- ë§ì„¤ì„, í”¼ë¡œê°, ì‹¤íŒ¨ ê²½í—˜ ë“±ì˜ ê°ì •ì„ ë‹¤ë£¨ê³ , ì‹¤ì²œì„ í–¥í•œ ë¯¸ì„¸í•œ ë™ê¸°ë¥¼ íƒìƒ‰í•˜ì„¸ìš”.\n"
            "- ì˜ˆ: 'ë³€í™”ë¥¼ ìƒê°í•  ë•Œ ì–´ë–¤ ë¶€ë‹´ì´ ë“œì‹œë‚˜ìš”?', 'ë¬´ì—‡ì´ ë§ì„¤ì´ê²Œ í•˜ë‚˜ìš”?', 'ì´ì „ ì‹œë„ì—ì„œ ë¬´ì—‡ì´ ì–´ë ¤ì› ë‚˜ìš”?'"
        )
    if enhanced:
        prompt += "\n- ìµœê·¼ ëŒ€í™” íë¦„ì´ ë°˜ë³µë˜ì—ˆê±°ë‚˜ ë°©í–¥ì´ ëª¨í˜¸í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”."
    return prompt

async def stream_mi_reply(state: AgentState, model_path: str) -> AsyncGenerator[bytes, None]:
    user_input = state.question.strip()

    if not user_input or len(user_input) < 2:
        fallback = "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"
        state.response = fallback
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "mi",
            "response": fallback,
            "history": state.history + [user_input, fallback],
            "drift_trace": state.drift_trace
        }, ensure_ascii=False).encode("utf-8")
        return

    try:
        llm = load_mi_model(model_path)

        # context ì„¤ì •
        if state.drift_trace and len(state.drift_trace) > 0:
            last_stage = state.drift_trace[-1][0]
            context = "cbt" if last_stage.startswith("cbt") else "empathy"
        else:
            context = "empathy"

        # enhanced ì¡°ê±´ ì•ˆì „ ì²˜ë¦¬
        enhanced = any(
            isinstance(item, (list, tuple)) and len(item) == 2 and item[0] == "mi" and item[1]
            for item in state.drift_trace[-5:]
        ) if state.drift_trace else False

        messages = [{"role": "system", "content": get_mi_prompt(context, enhanced)}]

        # âœ… ì•ˆì „í•œ ì§ êµ¬ì„±: zip ì‚¬ìš©í•˜ì—¬ ì§ì´ ì•ˆ ë§ëŠ” ê²½ìš° ë¬´ì‹œ
        history_pairs = list(zip(state.history[::2], state.history[1::2]))
        for user_msg, assistant_msg in history_pairs[-5:]:
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": assistant_msg})

        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        messages.append({"role": "user", "content": user_input})

        full_response, first_token_sent = "", False
        for chunk in llm.create_chat_completion(messages=messages, stream=True):
            token = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
            if token:
                full_response += token
                if not first_token_sent:
                    yield b"\n"
                    first_token_sent = True
                yield token.encode("utf-8")

        reply = full_response.strip() or "ê´œì°®ì•„ìš”. ë§ˆìŒì„ ì²œì²œíˆ ë“¤ë ¤ì£¼ì…”ë„ ê´œì°®ìŠµë‹ˆë‹¤."
        state.response = reply

        turn_count = len(state.history) // 2
        next_stage = "cbt1" if turn_count + 1 >= 5 else "mi"

        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": next_stage,
            "response": reply,
            "history": state.history + [user_input, reply],
            "drift_trace": state.drift_trace
        }, ensure_ascii=False).encode("utf-8")

    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
        fallback = "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        state.response = fallback
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "mi",
            "response": fallback,
            "history": state.history + [user_input, fallback],
            "drift_trace": state.drift_trace
        }, ensure_ascii=False).encode("utf-8")
