import os, json, multiprocessing, re, asyncio
from typing import AsyncGenerator, Literal, List
from pydantic import BaseModel, Field
from llama_cpp import Llama

LLM_CBT3_INSTANCE = {}

def load_cbt3_model(model_path: str) -> Llama:
    global LLM_CBT3_INSTANCE
    if model_path not in LLM_CBT3_INSTANCE:
        print("ğŸš€ CBT3 ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì¤‘...", flush=True)
        NUM_THREADS = max(1, multiprocessing.cpu_count() - 1)
        LLM_CBT3_INSTANCE[model_path] = Llama(
            model_path=model_path,
            n_ctx=1024,
            n_threads=NUM_THREADS,
            n_batch=4,
            max_tokens=128,
            temperature=0.65,
            top_p=0.9,
            presence_penalty=1.0,
            frequency_penalty=0.8,
            repeat_penalty=1.1,
            n_gpu_layers=0,
            low_vram=True,
            use_mlock=False,
            verbose=False,
            chat_format="llama-3",
            stop=["<|im_end|>", "\n\n"]
        )
    return LLM_CBT3_INSTANCE[model_path]

class AgentState(BaseModel):
    stage: Literal["cbt3", "end"]
    question: str
    response: str
    history: List[str]
    turn: int
    preset_questions: List[str] = Field(default_factory=list)
    drift_trace: List = Field(default_factory=list)

# âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸

def get_cbt3_prompt(enhanced=False) -> str:
    prompt = (
        "ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ë…¼ë¦¬ì ì¸ CBT ìƒë‹´ìì…ë‹ˆë‹¤.\n"
        "- ì‚¬ìš©ìì˜ ì‹¤ì²œ ê³„íšì„ íƒìƒ‰í•˜ëŠ” ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±í•´ ì£¼ì„¸ìš”.\n"
        "- ì§ˆë¬¸ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, ì‹¤ì œ í–‰ë™ì„ ìœ ë„í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "- ë°©í•´ ìš”ì¸, ê°ì • ë³€í™”, ìŠµê´€ í˜•ì„±, í™˜ê²½ ì¡°ì •, ìê¸° í”¼ë“œë°± ë“±ì— ì´ˆì ì„ ë§ì¶”ì–´ ì£¼ì„¸ìš”."
    )
    if enhanced:
        prompt += (
            "\n- ìµœê·¼ ëŒ€í™”ê°€ ë°˜ë³µë˜ê±°ë‚˜ ë°©í–¥ì´ ëª¨í˜¸í–ˆìŠµë‹ˆë‹¤. \n"
            "ë” êµ¬ì²´ì ì¸ ì‹¤ì²œ ê³„íšì„ ëŒì–´ë‚¼ ìˆ˜ ìˆë„ë¡ í•´ì£¼ì„¸ìš”."
        )
    return prompt

# âœ… CBT3 ë©€í‹°í„´ ì‘ë‹µ ìƒì„±ê¸°
async def stream_cbt3_reply(state: AgentState, model_path: str) -> AsyncGenerator[bytes, None]:
    try:
        llm = load_cbt3_model(model_path)
        enhanced = any(s == "cbt3" and d for s, d in getattr(state, "drift_trace", [])[-5:])
        prompt = get_cbt3_prompt(enhanced)

        messages = [{"role": "system", "content": prompt}]

        for i in range(0, len(state.history), 2):
            if i + 1 < len(state.history):
                messages.append({"role": "user", "content": state.history[i]})
                messages.append({"role": "assistant", "content": state.history[i + 1]})

        messages.append({"role": "user", "content": state.question})

        full_response = ""
        first_token_sent = False

        for chunk in llm.create_chat_completion(messages=messages, stream=True):
            await asyncio.sleep(0.015)
            token = chunk["choices"][0]["delta"].get("content", "")
            if token:
                full_response += token
                if not first_token_sent:
                    yield b"\n"
                    first_token_sent = True
                yield token.encode("utf-8")

        reply = full_response.strip()
        if not reply.endswith("?"):
            reply = reply.split(".")[0].strip() + "?"

        state.response = reply
        updated_history = state.history + [state.question, reply]
        next_turn = state.turn + 1
        next_stage = "end" if next_turn >= 5 else "cbt3"

        if next_stage == "end":
            end_msg = "\n\nğŸ¯ ì‹¤ì²œ ê³„íšì„ ì˜ ì •ë¦¬í•´ì£¼ì…¨ì–´ìš”. ì´ì œ ì˜¤ëŠ˜ ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í• ê²Œìš”."
            for ch in end_msg:
                yield ch.encode("utf-8")
                await asyncio.sleep(0.015)

        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": next_stage,
            "turn": next_turn if next_stage != "end" else 0,
            "response": reply,
            "history": updated_history,
            "preset_questions": state.preset_questions
        }, ensure_ascii=False).encode("utf-8")

    except Exception as e:
        print(f"âš ï¸ CBT3 ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
        fallback = "ì£„ì†¡í•´ìš”. ì§€ê¸ˆì€ ì ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì´ì•¼ê¸°í•´ ì£¼ì‹œê² ì–´ìš”?"
        state.response = fallback
        for ch in fallback:
            yield ch.encode("utf-8")
            await asyncio.sleep(0.02)
