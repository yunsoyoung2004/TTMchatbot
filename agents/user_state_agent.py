import os, json, multiprocessing, difflib
from typing import AsyncGenerator, Literal, List, Optional, Tuple
from pydantic import BaseModel
from llama_cpp import Llama
from drift.detector import run_detect
import nltk

# âœ… NLTK ë¦¬ì†ŒìŠ¤ ìë™ ë‹¤ìš´ë¡œë“œ
for resource in ["punkt", "averaged_perceptron_tagger"]:
    try:
        nltk.data.find(f'taggers/{resource}' if "tagger" in resource else f'tokenizers/{resource}')
    except LookupError:
        nltk.download(resource, quiet=True)

# âœ… DETECT ëª¨ë¸ ìºì‹œ
LLM_DETECT_INSTANCE = {}

def load_detect_model(model_path: str) -> Llama:
    global LLM_DETECT_INSTANCE
    if model_path not in LLM_DETECT_INSTANCE:
        print(f"ğŸ“¦ DETECT ëª¨ë¸ ë¡œë”©: {model_path}", flush=True)
        NUM_THREADS = max(1, multiprocessing.cpu_count() - 1)
        LLM_DETECT_INSTANCE[model_path] = Llama(
            model_path=model_path,
            n_ctx=1024,
            n_threads=NUM_THREADS,
            n_batch=8,
            max_tokens=128,
            temperature=0.95,
            top_p=0.92,
            presence_penalty=1.4,
            frequency_penalty=1.2,
            repeat_penalty=1.3,
            n_gpu_layers=0,
            low_vram=True,
            use_mlock=False,
            verbose=False,
            chat_format="llama-3",
            stop=["<|im_end|>"]
        )
    return LLM_DETECT_INSTANCE[model_path]

# âœ… ìƒíƒœ ëª¨ë¸
class AgentState(BaseModel):
    stage: Literal["cbt1", "cbt2"]
    question: str
    response: str
    history: List[str]
    turn: int
    drift_trace: List[Tuple[str, bool]] = []

# âœ… í”„ë¡¬í”„íŠ¸ ìƒì„±
def get_detect_prompt(history: List[str]) -> str:
    joined = "\n".join(history[-8:])
    return (
        "ë‹¤ìŒì€ ìƒë‹´ì(ì±—ë´‡)ì™€ ì‚¬ìš©ì ê°„ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤.\n"
        "ë‹¹ì‹ ì€ ì „ë¬¸ ì‹¬ë¦¬ìƒë‹´ê°€ë¡œì„œ, ì´ ëŒ€í™” íë¦„ì„ í‰ê°€í•˜ì—¬ ì‚¬ìš©ìì˜ ìƒíƒœì™€ íë¦„ ì í•©ì„±ì„ ìš”ì•½í•˜ê³ ,\n"
        "ë§ˆì§€ë§‰ ì¤„ì— [ì˜ˆ] ë˜ëŠ” [ì•„ë‹ˆì˜¤]ë¡œ 'ìƒë‹´ íë¦„ ì „í™˜ì´ í•„ìš”í•˜ë‹¤'ê³  íŒë‹¨ë˜ëŠ”ì§€ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.\n"
        "\n"
        "- ê°ì • í‘œí˜„ ë°©ì‹ê³¼ ì¼ê´€ì„±\n"
        "- ì£¼ì œ ì¤‘ì‹¬ì„±\n"
        "- ì‹¤ì²œ/ë³€í™” ì˜ì§€\n"
        "- íë¦„ ì í•©ì„± (í˜„ì¬ ìƒë‹´ íë¦„ì´ ì ì ˆí•œì§€)\n"
        "\n# ëŒ€í™” ë‚´ìš©:\n"
        f"{joined}\n"
        "\n# ìƒë‹´ì ìš”ì•½:\n"
    )

# âœ… ì‚¬ìš©ì ìƒíƒœ í‰ê°€ ìˆ˜í–‰
def evaluate_user_state(state: AgentState, model_path: str) -> Tuple[str, bool]:
    prompt = get_detect_prompt(state.history)
    model = load_detect_model(model_path)
    try:
        response_obj = model.create_chat_completion(
            messages=[{"role": "user", "content": prompt}]
        )
        response = response_obj.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
    except Exception as e:
        print(f"âš ï¸ ìƒíƒœ í‰ê°€ ì‹¤íŒ¨: {e}")
        return "ìš”ì•½ ì‹¤íŒ¨", False

    summary = response or "ìš”ì•½ ì‹¤íŒ¨"
    rollback = summary.endswith("ì˜ˆ")
    return summary, rollback

# âœ… ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
async def run_user_state_agent(state: AgentState, model_path: str, mode="drift_profile"):
    if mode == "plain":
        return {}

    drifted = run_detect(state)

    if mode == "drift_only":
        return {"enhanced": drifted}

    if drifted:
        summary, rollback = evaluate_user_state(state, model_path)
        print(f"[DRIFT-EVAL] ìš”ì•½:\n{summary}\nâ†’ MIë¡œ ì „í™˜ í•„ìš”? {rollback}")
        return {
            "need_rollback": rollback,
            "summary": summary
        }

    return {}
