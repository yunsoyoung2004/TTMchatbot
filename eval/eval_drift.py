import json
import time
from sklearn.metrics import precision_score, recall_score, f1_score
from agents.user_state_agent import AgentState
from drift.detector import pure_run_detect  # âœ… í‰ê°€ìš© ê°ì§€ê¸°
from shared.logger import logger

async def evaluate_drift_detection():
    try:
        with open('eval/evaluation.json', 'r', encoding='utf-8') as f:
            examples = json.load(f)
        logger.info(f"ğŸ“‚ Drift í‰ê°€ ìƒ˜í”Œ ìˆ˜: {len(examples)}")
    except FileNotFoundError:
        logger.error("âŒ evaluation.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    y_true, y_pred, scores, times = [], [], [], []

    for idx, ex in enumerate(examples):
        text = ex["text"]
        expected = bool(ex["should_transition"])  # âœ… ì •ë‹µ í™•ì‹¤íˆ bool ë³€í™˜
        previous = ex.get("previous_text", "")

        state = AgentState(
            stage="cbt1",
            question="",
            response=text,
            history=[previous, text],
            turn=0,
            drift_trace=[]
        )

        start = time.time()
        try:
            result = pure_run_detect(state)
        except Exception as e:
            logger.warning(f"âŒ {idx+1}ë²ˆ ì˜ˆì‹œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

        elapsed = time.time() - start
        predicted = bool(result.get("drift", False))  # âœ… ì˜ˆì¸¡ë„ í™•ì‹¤íˆ bool ë³€í™˜
        score = result.get("score", 0.0)

        logger.info(f"âœ… {idx+1}/{len(examples)} - ì˜ˆì¸¡: {predicted} (ì •ë‹µ: {expected}) | ì ìˆ˜: {score:.4f} | ì‹œê°„: {elapsed:.3f}s")

        y_true.append(expected)
        y_pred.append(predicted)
        scores.append(score)
        times.append(elapsed)

    if not y_true:
        logger.warning("âš ï¸ í‰ê°€ì— ì‚¬ìš©í•  ìœ íš¨í•œ ì˜ˆì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    avg_score = sum(scores) / len(scores) if scores else 0.0
    avg_time = sum(times) / len(times) if times else 0.0

    result_summary = {
        "precision": round(precision, 3),
        "recall": round(recall, 3),
        "f1_score": round(f1, 3),
        "avg_drift_score": round(avg_score, 4),
        "avg_response_time": round(avg_time, 4)
    }

    logger.info("ğŸ“Š Drift Detection ìµœì¢… í‰ê°€ ê²°ê³¼")
    for k, v in result_summary.items():
        logger.info(f"{k}: {v}")

    return result_summary
